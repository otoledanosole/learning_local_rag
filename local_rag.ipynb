{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/otoledanosole/learning_local_rag/blob/main/local_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creation of a Local Retrieval Augmented Genration (RAG)**\n",
        "\n",
        "Objective of the document"
      ],
      "metadata": {
        "id": "SrcJEbIGn99m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example of what we are triying to build**\n",
        "\n",
        "<img src=\"https://github.com/otoledanosole/learning_local_rag/blob/main/images/Nvidia_rag-pipeline-ingest-query-flow-b-2048x960.png?raw=true\" alt=\"Retrieval Augmented Generation (RAG) Sequence Diagram from Nvidia\" />\n",
        "\n",
        "We are going to use the post [\"RAG 101: Demystifying Retrieval Augmented Generation Pipelines\"](https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/) from the Nvidia Technical Blog as a reference of what we are going to build except that we are not going to use an exsistant framework.\n",
        "\n",
        "The objective is, instead of using LangChain or LlamaIndex as they do in the example, we are going to build our own framework in order to really understand what a RAG is.\n"
      ],
      "metadata": {
        "id": "jFesvf_IojUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is RAG?**\n",
        "\n",
        "RAG stands for Retrieval Augmented Generation\n",
        "\n",
        "RAG is a technique for enhancing the acuracy and reliability of generative AI models with facts fetched from external resources.\n",
        "\n",
        "It was firstly introduced in a 202 paper called: [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401#)\n",
        "\n",
        "To understand it with a real example, imagine a psycologist:\n",
        ">A psycologist can create strategies or aply treatment to their patients based of their general understanding of diferent psycology techniques but sometimes a patient requires special expertise so psycologist send clerks to a library where they store specific studies they can use\n",
        "\n",
        "Like a good psycologyst, large language models (LLMs) can respond to a variety of queries  but if we look under de hood of a LLM we find a neural network with parametres that essentialy represent the general patterns of how humans use words to form sentences. Some pre-trained LLMs models have been shown to store factual knowledge in their parameters, working as a parameterized implicit knowledge base.\n",
        "\n",
        "While this is interesting, such models have downsides: the information that they are based of is dificult to update o revise, they may cause \"hallucinations\" and they can not provide in depth knowledge of a specific topic. Some hybrid models combine parametric and non-parametric memories.\n",
        "\n",
        "The goal of RAG is to take information from a source, pre-process it and pass it to an LLM so it can generate outputs based on that information.\n",
        "\n",
        "\n",
        ">To understand what retrieval-augmented generation means, we can roughly broke down eatch step to:\n",
        "* Retrieval - Find relevant information given a query, also known as prompt. For example: \"What pushes a person to act against his beliefs?\" → retrieves the information related to this topic from a given sorurce, for example the book \"Social psycology\" from Solomon Asch.\n",
        "* Augmented - Take the relevant information and augment our input to an LLM with that relevant information.\n",
        "* Generation - Take the firsts two steps and pass them to an LLM for generative outputs.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2XrtqOcXZlbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Benefits of RAG**\n",
        "\n",
        "The main advantages of using RAG are:\n",
        "- Empowering LLM solutions with real-time data access\n",
        "- Preserving data privacy\n",
        "- Mitigating LLM hallucinations\n",
        "\n",
        "**Empowering LLM solutions with real-time data access**\n",
        ">Generaly, LLMs trained with \"internet\" data have a fairly good understanding of languaje in general. However, it also means that the responses may be to generic for some aplications.\n",
        "Because data is constantly changing, using RAG, facilitates direct access to the resources than LLM models use. These resources can consist of real-time and personalized data.\n",
        "RAG helps to create especific responses based on specific data.\n",
        "\n",
        "**Preserving data privacy**\n",
        ">One of the main benefits of using RAG is the privacy. Because the LLM is self hosted, all the sensitive data is not exposed.\n",
        "\n",
        "**Mitigating LLM hallucinations**\n",
        ">LLMs are good at generating *good looking* text, however, this text doesen't mean is factual. We know as hallucination, hen a LLM provides faulty responses but in such a convincing way they sound real.\n",
        "RAG helps mitigate hallucinations.\n",
        "\n"
      ],
      "metadata": {
        "id": "fFDPAL1Fospo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Where we can use RAG?**\n",
        "\n",
        "The main usage for RAG is to take your relevant documents and convert them to a prompt to be used by a LLM.\n",
        "\n",
        "In fact, almost any business can turn its internal data into resources called knowledge bases and enhance LLMs.\n",
        "\n",
        "Som uses could be:\n",
        "- Email data analysis: Imagine that you work for a company that recibes tons and tons of emails and you need to search for a specific topic or conversations. You can use RAG to get all the relevant information of the emails and then use a LLM to generate a response.\n",
        "- Company internal knowledgebase: Another use could be to create a n internal knowledgebase. Imagine that you are working for a software company and you find a bug in a system. You can create a document on how to fix it and pass it to a RAG so your collegues can use a LLM to search for answers.\n",
        "- Text book reading: Imagine that you need to read a book about something. You can use RAG to go through the book and extract the relevant information."
      ],
      "metadata": {
        "id": "WOSf4f-1xwKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Why do you want to run it localy?**\n",
        "\n",
        "Lets go through a few arguments about why is a good idea to run it localy:\n",
        "- The first one is privacy of the data. If you are using sensitive or private data, maybe you dont want to send it through the internet to another company.\n",
        "- Another benefit is speed. You dont depend on network availability and you can use large amount of data without having to send it using an API.\n",
        "- We must also look at the cost. Running it localy may need a big initial investment but in the long run, the cost is paid. If aan external service is used, you have to pay API fees.\n",
        "- Another good argument is that you are not locked to a specific vendor if you run your own haardware/software. Imagine if you are using ChatGPT to run your system. If OpenAI/another comany shuts down, you can still run the system."
      ],
      "metadata": {
        "id": "UwbrfzYr3NA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What we are trying to build**\n",
        "\n",
        "Using the example made by [Daniel Bourke](https://github.com/mrdbourke/simple-local-rag/tree/main), we are going to build a RAG pipeline to chat with a PDF document.\n",
        "\n",
        "In our case, we are going to use an open source [nutrition textbook](https://pressbooks.oer.hawaii.edu/humannutrition2/).\n",
        "\n",
        "We are going to create a code for the next steps:\n",
        "\n",
        "\n",
        "1. Open a PDF document\n",
        "2. Format the text of the PDF textbook ready for an embedding model. This process is called text splitting/chinking\n",
        "3. Embed all of the chunks of text in the textbook and return them a into numerical representation which we can store later.\n",
        "4. Build a retrieval system that uses vector search to find relevant chunks of text based on a query.\n",
        "5. Create a prompt that incorporates the retrieved pieces of text.\n",
        "6. Generate an answer to a query based on passages from the textbook.\n",
        "\n",
        "We can classify the above steps in two major groups:\n",
        "1. Document processing and embeding creation (steps 1-3)\n",
        "2. Search and answer (steps 4-6)\n",
        "\n",
        "<img src=\"https://github.com/otoledanosole/learning_local_rag/blob/main/images/Learning_Local_RAG.png?raw=true\" alt=\"Retrieval Augmented Generation (RAG) Sequence Diagram\" />\n"
      ],
      "metadata": {
        "id": "sVPmKqm5WrCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Key words**\n",
        "\n",
        "**TOKEN**\n",
        "\n",
        "\n",
        ">A sub-word piece of text. For example, \"hello, world!\" could be split into [\"hello\", \",\", \"world\", \"!\"]. A token can be a whole word, part of a word or group of punctuation characters. 1 token ~= 4 characters in English, 100 tokens ~= 75 words.<br>\n",
        "Text gets broken into tokens before being passed to an LLM.<br>\n",
        "Before words go to an LLM, for example, ChatGPT, words get tokenized. We can find more information about what is a token here: [What is a token end how to count them](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)<br>\n",
        "With OpenAi [Tokenizer](https://platform.openai.com/tokenizer) tool, we can see how a text is converted into tokens:<br>\n",
        "<img src=\"https://github.com/otoledanosole/learning_local_rag/blob/main/images/Tokenized_Hello_World.png?raw=true\" alt=\"Tokenization of words\"/>|"
      ],
      "metadata": {
        "id": "rSoRxsaX0g-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Requirements and setup**\n",
        "\n",
        "Because we are going to use Google Colab to run our RAG, we need to do some initial configurations:"
      ],
      "metadata": {
        "id": "LKM-9OP3Xz8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os                                                 # Python for os module\n",
        "\n",
        "if \"COLAB_GPU\" in os.environ:\n",
        "    print(\"[INFO] Running in Google Colab, installing requirements.\")\n",
        "    !pip install -U torch                                 # requires torch 2.1.1+ (for efficient sdpa implementation)\n",
        "    !pip install PyMuPDF                                  # for reading PDFs with Python\n",
        "    !pip install tqdm                                     # for progress bars\n",
        "    !pip install sentence-transformers                    # for embedding models\n",
        "    !pip install accelerate                               # for quantization model loading\n",
        "    !pip install bitsandbytes                             # for quantizing models (less storage space)\n",
        "    !pip install flash-attn --no-build-isolation          # for faster attention mechanism = faster LLM inference"
      ],
      "metadata": {
        "collapsed": true,
        "id": "HEeTeBarYsgz",
        "outputId": "4cf542fa-fb70-4f84-8ae2-769e6038d53b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Running in Google Colab, installing requirements.\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.47.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.27.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.27.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.5)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.12.14)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: typing_extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n",
            "Requirement already satisfied: flash-attn in /usr/local/lib/python3.10/dist-packages (2.7.2.post1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.5.1+cu121)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Document processing and embeding creation**\n",
        "\n",
        "For the first part of the RAG creation we need:\n",
        "- PDF document of choice\n",
        "- Embeding model of choice\n",
        "\n",
        "The steps that we are going to follow are:\n",
        "1. Import PDF document\n",
        "2. Process text for embeding (Split into chunks of sentences)\n",
        "3. Embed text chunks with embeding model\n",
        "4. Save embedings for later use"
      ],
      "metadata": {
        "id": "PhGuFw-jbdJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import PDF document\n",
        "\n",
        "We are going to satrt with a PDF but this will work with diferent kinds of documents like text files, email chains, support documentation, articles from the internet, etc.\n",
        "\n",
        "We are going to use PyMuPDF as the library to open the PDFs.\n",
        "\n",
        "First we'll download the PDF if it doesn't exist."
      ],
      "metadata": {
        "id": "-licqJ9EdC5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "# Obtain the PDF Path\n",
        "pdf_path = \"Human-Nutrition-2020-Edition.pdf\"\n",
        "\n",
        "# Download PDF\n",
        "if not os.path.exists(pdf_path): #The first thing we do is check if the file exists\n",
        "  print(f\"[INFO] File doesn't exist, downloading...\")\n",
        "\n",
        "  #Enter  the URL of the PDF\n",
        "  url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"  #This has to be the DOWNLOAD URL. If not, it's not gona give us the PDF\n",
        "\n",
        "  # The local file name to save the downloaded file\n",
        "  filename = pdf_path #We can use the same name for the file as the pdf path\n",
        "\n",
        "  #Send a GET request to the URL\n",
        "  response = requests.get(url) #We use the request library to send a GET to the url\n",
        "\n",
        "  #Check if the request was succesful\n",
        "  #You can chech the response status code in the next page: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status\n",
        "  if response.status_code == 200: #200 value means OK, because we are using a GET request --> 200 = The resource has been fetched and transmitted in the message body.\n",
        "    #Open the file and save if\n",
        "    with open(filename, \"wb\") as file:\n",
        "      file.write(response.content)\n",
        "    print(f\"[INFO] The file has been download and saved as {filename}\")\n",
        "  else:\n",
        "    print(f\"[INFO] Failed to download the file. Status code: {response.status_code}\")\n",
        "\n",
        "else:\n",
        "  print(f\"[INFO] The file {pdf_path} already exists\")\n"
      ],
      "metadata": {
        "id": "hmmz781cd7DW",
        "outputId": "ef68c909-b5c3-4e42-adbc-a81abdade6ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] The file Human-Nutrition-2020-Edition.pdf already exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PDF Downloaded!\n",
        "\n",
        "Now we ned to open it. We are going to use [PyMuPDF](\"https://github.com/pymupdf/PyMuPDF\") (import fitz) to open and read our PDF document.\n",
        "\n",
        "The small function that we are going to build is to read a PDF. Not all the texts are read the same.\n",
        "\n",
        "We'll save each page to a dictionary and then append that dictionary to a list for ease of use later."
      ],
      "metadata": {
        "id": "HAA7J8pHoGKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz # Requires !pip install PyMuPDF, see: https://github.com/pymupdf/pymupdf\n",
        "from tqdm.auto import tqdm # pip install tqdm\n",
        "\n",
        "# We are going to define helper functions\n",
        "\n",
        "\"\"\"\n",
        "This function perfoms a formatting over the text\n",
        "\n",
        "  Parametres:\n",
        "    text (str): string of text\n",
        "\n",
        "  Returns:\n",
        "    cleaned_text (str): the same text but with the formatting aplied\n",
        "\"\"\"\n",
        "def text_formatter(text: str) -> str:\n",
        "  \"\"\"\n",
        "  When you import text in some way, its important to check the format of the text because it may not be perfect word for word.\n",
        "  Thats why its important to do som text formatting and write some preproccessing steps.\n",
        "  In our case with the PDF:\n",
        "\n",
        "  We are going to replace new lines (\"\\n\") with space (\" \") and we are going to stip the white spaces on the end (.strip())\n",
        "  \"\"\"\n",
        "\n",
        "  cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "  \"\"\"\n",
        "  Its important to check and test the necessary formatting for each text\n",
        "  So we may need more text formatting functions\n",
        "\n",
        "  ¡¡  Better formatting = Better text = Better and more acurate responses from the LLM  !!\n",
        "  \"\"\"\n",
        "  return cleaned_text\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "This function opens and reads every page/line from the PDF\n",
        "The function only focuses on text, no images or other elements.\n",
        "The objective of the function is to open a PDF file, read its text page by page and collect statistics.\n",
        "\n",
        "    Parameters:\n",
        "        pdf_path (str): The file path to the PDF document to be opened and read.\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: A list of dictionaries, each containing the page number\n",
        "        (adjusted), character count, word count, sentence count, token count, and the extracted text\n",
        "        for each page.\n",
        "\n",
        "\"\"\"\n",
        "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
        "  doc = fitz.open(pdf_path) #Open the PDF with PyMuPDF\n",
        "  pages_and_texts = [] #We create an empty list\n",
        "\n",
        "  for page_number, page in tqdm(enumerate(doc)):    #tqdm is a progress bar\n",
        "      text = page.get_text()  #Get the text from the page\n",
        "      text = text_formatter(text) #Format the text\n",
        "      pages_and_texts.append({\"page_number\": page_number - 41,  #In our case, the page numbers on the document starts on page 43. Take in mind that the page number may not match the original PDF\n",
        "                              \"page_char_count\": len(text), #How many characters we have on the text\n",
        "                              \"page_word_count\": len(text.split(\" \")),  #How many words we have on the text (roughly)\n",
        "                              \"page_sentence_count_raw\": len(text.split(\". \")), #How many sentences we have on the text (roughly)\n",
        "                              \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
        "                              \"text\": text})\n",
        "\n",
        "  return pages_and_texts\n",
        "\n",
        "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
        "pages_and_texts[:2] #We get the first two samples"
      ],
      "metadata": {
        "id": "YEtMsLTFrIEB",
        "outputId": "b806496d-21b4-47c5-d5af-0cee9a75fea7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257,
          "referenced_widgets": [
            "bd38ca91260241499152baec940354b1",
            "7d7e84037afa45c5a20e01d5f785fc2b",
            "ca5492920c6247b8a73d167250a3634e",
            "8a90e08b1ff742a3bb0746c73873a91d",
            "27a09246c7104b6ba323fc3fd587488d",
            "d9cd2dcb5e8c4549bcdbbddddc6f107a",
            "2c150ca6565744fa812e3fbc3fd04a17",
            "4bafbcb68e0d48d8990e048859a0e8ee",
            "f81b8ae431a94348a6a4b373d09752ae",
            "0aa81c414da04c1caded9521fe45bbc9",
            "8a1da69c9be144b1b0636dd590d2d906"
          ]
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd38ca91260241499152baec940354b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'page_number': -41,\n",
              "  'page_char_count': 29,\n",
              "  'page_word_count': 4,\n",
              "  'page_sentence_count_raw': 1,\n",
              "  'page_token_count': 7.25,\n",
              "  'text': 'Human Nutrition: 2020 Edition'},\n",
              " {'page_number': -40,\n",
              "  'page_char_count': 0,\n",
              "  'page_word_count': 1,\n",
              "  'page_sentence_count_raw': 1,\n",
              "  'page_token_count': 0.0,\n",
              "  'text': ''}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random #Import random library\n",
        "\n",
        "random.sample(pages_and_texts, k=3) #We obtain 3 random pages from our list of dictionaries"
      ],
      "metadata": {
        "id": "TjxIclfE8L6O",
        "outputId": "1fcb36c0-78ed-438a-9dfd-58330acad787",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'page_number': 713,\n",
              "  'page_char_count': 1499,\n",
              "  'page_word_count': 282,\n",
              "  'page_sentence_count_raw': 14,\n",
              "  'page_token_count': 374.75,\n",
              "  'text': 'Image by  Allison  Calabrese /  CC BY 4.0  been sufficient scientific research into the particular  nutritional requirements for infants. Consequently, all of the  DRI values for infants are AIs derived from nutrient values in  human breast milk. For older babies and children, AI values are  derived from human milk coupled with data on adults. The AI is  meant for a healthy target group and is not meant to be  sufficient for certain at-risk groups, such as premature infants.  2. Tolerable Upper Intake Levels. The UL was established to help  distinguish healthful and harmful nutrient intakes. Developed  in part as a response to the growing usage of dietary  supplements, ULs indicate the highest level of continuous  intake of a particular nutrient that may be taken without  causing health problems. When a nutrient does not have any  known issue if taken in excessive doses, it is not assigned a UL.  However, even when a nutrient does not have a UL it is not  necessarily safe to consume in large amounts.  Figure 12.1 DRI Graph  This graph illustrates the risks of nutrient inadequacy and nutrient  excess as we move from a low intake of a nutrient to a high intake.  Starting on the left side of the graph, you can see that when you  have a very low intake of a nutrient, your risk of nutrient deficiency  is high. As your nutrient intake increases, the chances that you  will be deficient in that nutrient decrease. The point at which 50  Understanding Dietary Reference Intakes  |  713'},\n",
              " {'page_number': 447,\n",
              "  'page_char_count': 769,\n",
              "  'page_word_count': 124,\n",
              "  'page_sentence_count_raw': 4,\n",
              "  'page_token_count': 192.25,\n",
              "  'text': 'Learning Activities  Technology Note: The second edition of the Human  Nutrition Open Educational Resource (OER) textbook  features interactive learning activities.\\xa0 These activities are  available in the web-based textbook and not available in the  downloadable versions (EPUB, Digital PDF, Print_PDF, or  Open Document).  Learning activities may be used across various mobile  devices, however, for the best user experience it is strongly  recommended that users complete these activities using a  desktop or laptop computer and in Google Chrome.  \\xa0 An interactive or media element has been  excluded from this version of the text. You can  view it online here:  http://pressbooks.oer.hawaii.edu/ humannutrition2/?p=287  \\xa0 Health Consequences of Alcohol Abuse  |  447'},\n",
              " {'page_number': 681,\n",
              "  'page_char_count': 1440,\n",
              "  'page_word_count': 232,\n",
              "  'page_sentence_count_raw': 9,\n",
              "  'page_token_count': 360.0,\n",
              "  'text': 'Iodine  UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND HUMAN  NUTRITION PROGRAM AND HUMAN NUTRITION PROGRAM  Recall the discovery of iodine and its use as a means of preventing  goiter, a gross enlargement of the thyroid gland in the neck. Iodine  is essential for the synthesis of thyroid hormone, which regulates  basal metabolism, growth, and development. Low iodine levels and  consequently hypothyroidism has many signs and symptoms  including fatigue, sensitivity to cold, constipation, weight gain,  depression, and dry, itchy skin and paleness. The development of  goiter may often be the most visible sign of chronic iodine  deficiency, but the consequences of low levels of thyroid hormone  can be severe during infancy, childhood, and adolescence as it  affects all stages of growth and development. Thyroid hormone  plays a major role in brain development and growth and fetuses  and infants with severe iodine deficiency develop a condition known  as cretinism, in which physical and neurological impairment can  be severe. The World Health Organization (WHO) estimates iodine  deficiency affects over two billion people worldwide and it is the  number-one cause of preventable brain damage worldwide.1  Figure 11.5 Deaths Due to Iodine Deficiency Worldwide in 2012  1.\\xa0World Health Organization. “Iodine Status Worldwide.”  Accessed October 2, 2011. http://whqlibdoc.who.int/ publications/2004/9241592001.pdf.  Iodine  |  681'}]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sources\n",
        "\n",
        "https://arxiv.org/abs/2005.11401#\n",
        "\n",
        "https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\n",
        "\n",
        "https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/"
      ],
      "metadata": {
        "id": "tdmF69mMeBwK"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Te damos la bienvenida a Colaboratory",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bd38ca91260241499152baec940354b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d7e84037afa45c5a20e01d5f785fc2b",
              "IPY_MODEL_ca5492920c6247b8a73d167250a3634e",
              "IPY_MODEL_8a90e08b1ff742a3bb0746c73873a91d"
            ],
            "layout": "IPY_MODEL_27a09246c7104b6ba323fc3fd587488d"
          }
        },
        "7d7e84037afa45c5a20e01d5f785fc2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9cd2dcb5e8c4549bcdbbddddc6f107a",
            "placeholder": "​",
            "style": "IPY_MODEL_2c150ca6565744fa812e3fbc3fd04a17",
            "value": ""
          }
        },
        "ca5492920c6247b8a73d167250a3634e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bafbcb68e0d48d8990e048859a0e8ee",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f81b8ae431a94348a6a4b373d09752ae",
            "value": 1
          }
        },
        "8a90e08b1ff742a3bb0746c73873a91d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0aa81c414da04c1caded9521fe45bbc9",
            "placeholder": "​",
            "style": "IPY_MODEL_8a1da69c9be144b1b0636dd590d2d906",
            "value": " 1208/? [00:02&lt;00:00, 509.05it/s]"
          }
        },
        "27a09246c7104b6ba323fc3fd587488d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9cd2dcb5e8c4549bcdbbddddc6f107a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c150ca6565744fa812e3fbc3fd04a17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bafbcb68e0d48d8990e048859a0e8ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f81b8ae431a94348a6a4b373d09752ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0aa81c414da04c1caded9521fe45bbc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a1da69c9be144b1b0636dd590d2d906": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}