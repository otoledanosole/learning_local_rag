{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/otoledanosole/learning_local_rag/blob/main/local_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creation of a Local Retrieval Augmented Genration (RAG)**\n",
        "\n",
        "Objective of the document"
      ],
      "metadata": {
        "id": "SrcJEbIGn99m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example of what we are triying to build**\n",
        "\n",
        "<img src=\"https://github.com/otoledanosole/learning_local_rag/blob/main/images/Nvidia_rag-pipeline-ingest-query-flow-b-2048x960.png?raw=true\" alt=\"Retrieval Augmented Generation (RAG) Sequence Diagram from Nvidia\" />\n",
        "\n",
        "We are going to use the post [\"RAG 101: Demystifying Retrieval Augmented Generation Pipelines\"](https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/) from the Nvidia Technical Blog as a reference of what we are going to build except that we are not going to use an exsistant framework.\n",
        "\n",
        "The objective is, instead of using LangChain or LlamaIndex as they do in the example, we are going to build our own framework in order to really understand what a RAG is.\n"
      ],
      "metadata": {
        "id": "jFesvf_IojUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is RAG?**\n",
        "\n",
        "RAG stands for Retrieval Augmented Generation\n",
        "\n",
        "RAG is a technique for enhancing the acuracy and reliability of generative AI models with facts fetched from external resources.\n",
        "\n",
        "It was firstly introduced in a 202 paper called: [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401#)\n",
        "\n",
        "To understand it with a real example, imagine a psycologist:\n",
        ">A psycologist can create strategies or aply treatment to their patients based of their general understanding of diferent psycology techniques but sometimes a patient requires special expertise so psycologist send clerks to a library where they store specific studies they can use\n",
        "\n",
        "Like a good psycologyst, large language models (LLMs) can respond to a variety of queries  but if we look under de hood of a LLM we find a neural network with parametres that essentialy represent the general patterns of how humans use words to form sentences. Some pre-trained LLMs models have been shown to store factual knowledge in their parameters, working as a parameterized implicit knowledge base.\n",
        "\n",
        "While this is interesting, such models have downsides: the information that they are based of is dificult to update o revise, they may cause \"hallucinations\" and they can not provide in depth knowledge of a specific topic. Some hybrid models combine parametric and non-parametric memories.\n",
        "\n",
        "The goal of RAG is to take information from a source, pre-process it and pass it to an LLM so it can generate outputs based on that information.\n",
        "\n",
        "\n",
        ">To understand what retrieval-augmented generation means, we can roughly broke down eatch step to:\n",
        "* Retrieval - Find relevant information given a query, also known as prompt. For example: \"What pushes a person to act against his beliefs?\" â†’ retrieves the information related to this topic from a given sorurce, for example the book \"Social psycology\" from Solomon Asch.\n",
        "* Augmented - Take the relevant information and augment our input to an LLM with that relevant information.\n",
        "* Generation - Take the firsts two steps and pass them to an LLM for generative outputs.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2XrtqOcXZlbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Benefits of RAG**\n",
        "\n",
        "The main advantages of using RAG are:\n",
        "- Empowering LLM solutions with real-time data access\n",
        "- Preserving data privacy\n",
        "- Mitigating LLM hallucinations\n",
        "\n",
        "**Empowering LLM solutions with real-time data access**\n",
        ">Generaly, LLMs trained with \"internet\" data have a fairly good understanding of languaje in general. However, it also means that the responses may be to generic for some aplications.\n",
        "Because data is constantly changing, using RAG, facilitates direct access to the resources than LLM models use. These resources can consist of real-time and personalized data.\n",
        "RAG helps to create especific responses based on specific data.\n",
        "\n",
        "**Preserving data privacy**\n",
        ">One of the main benefits of using RAG is the privacy. Because the LLM is self hosted, all the sensitive data is not exposed.\n",
        "\n",
        "**Mitigating LLM hallucinations**\n",
        ">LLMs are good at generating *good looking* text, however, this text doesen't mean is factual. We know as hallucination, hen a LLM provides faulty responses but in such a convincing way they sound real.\n",
        "RAG helps mitigate hallucinations.\n",
        "\n"
      ],
      "metadata": {
        "id": "fFDPAL1Fospo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Where we can use RAG?**\n",
        "\n",
        "The main usage for RAG is to take your relevant documents and convert them to a prompt to be used by a LLM.\n",
        "\n",
        "In fact, almost any business can turn its internal data into resources called knowledge bases and enhance LLMs.\n",
        "\n",
        "Som uses could be:\n",
        "- Email data analysis: Imagine that you work for a company that recibes tons and tons of emails and you need to search for a specific topic or conversations. You can use RAG to get all the relevant information of the emails and then use a LLM to generate a response.\n",
        "- Company internal knowledgebase: Another use could be to create a n internal knowledgebase. Imagine that you are working for a software company and you find a bug in a system. You can create a document on how to fix it and pass it to a RAG so your collegues can use a LLM to search for answers.\n",
        "- Text book reading: Imagine that you need to read a book about something. You can use RAG to go through the book and extract the relevant information."
      ],
      "metadata": {
        "id": "WOSf4f-1xwKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Why do you want to run it localy?**\n",
        "\n",
        "Lets go through a few arguments about why is a good idea to run it localy:\n",
        "- The first one is privacy of the data. If you are using sensitive or private data, maybe you dont want to send it through the internet to another company.\n",
        "- Another benefit is speed. You dont depend on network availability and you can use large amount of data without having to send it using an API.\n",
        "- We must also look at the cost. Running it localy may need a big initial investment but in the long run, the cost is paid. If aan external service is used, you have to pay API fees.\n",
        "- Another good argument is that you are not locked to a specific vendor if you run your own haardware/software. Imagine if you are using ChatGPT to run your system. If OpenAI/another comany shuts down, you can still run the system."
      ],
      "metadata": {
        "id": "UwbrfzYr3NA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What we are trying to build**\n",
        "\n",
        "Using the example made by [Daniel Bourke](https://github.com/mrdbourke/simple-local-rag/tree/main), we are going to build a RAG pipeline to chat with a PDF document.\n",
        "\n",
        "In our case, we are going to use an open source [nutrition textbook](https://pressbooks.oer.hawaii.edu/humannutrition2/).\n",
        "\n",
        "We are going to create a code for the next steps:\n",
        "\n",
        "\n",
        "1. Open a PDF document\n",
        "2. Format the text of the PDF textbook ready for an embedding model. This process is called text splitting/chinking\n",
        "3. Embed all of the chunks of text in the textbook and return them a into numerical representation which we can store later.\n",
        "4. Build a retrieval system that uses vector search to find relevant chunks of text based on a query.\n",
        "5. Create a prompt that incorporates the retrieved pieces of text.\n",
        "6. Generate an answer to a query based on passages from the textbook.\n",
        "\n",
        "We can classify the above steps in two major groups:\n",
        "1. Document processing and embeding creation (steps 1-3)\n",
        "2. Search and answer (steps 4-6)\n",
        "\n",
        "<img src=\"https://github.com/otoledanosole/learning_local_rag/blob/main/images/Learning_Local_RAG.png?raw=true\" alt=\"Retrieval Augmented Generation (RAG) Sequence Diagram\" />\n"
      ],
      "metadata": {
        "id": "sVPmKqm5WrCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sources\n",
        "\n",
        "https://arxiv.org/abs/2005.11401#\n",
        "\n",
        "https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/\n",
        "\n",
        "https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/"
      ],
      "metadata": {
        "id": "tdmF69mMeBwK"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Te damos la bienvenida a Colaboratory",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}